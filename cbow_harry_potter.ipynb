{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow_harry_potter.ipynb  harry_potter.txt\t models       word2vec.h5\r\n",
      "CBOW_keras.ipynb\t India_after_gandhi.txt  Reviews.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_fwf('harry_potter.txt',names = ['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48305,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.Text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Shut your mouth!\" bellowed Ron, bypassing red and turning maroon.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.Text[45090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    \n",
    "    # remove special characters and digits if any\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "corpus['Text'] = corpus['Text'].apply(lambda x:pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nearly twice the usual amount of neck which came in very useful as she'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.Text[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/anaconda3/envs/Tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 16696\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus.Text)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "# build vocabulary of unique words\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in corpus.Text]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 200\n",
    "window_size = 5\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 200)           3339200   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16696)             3355896   \n",
      "=================================================================\n",
      "Total params: 6,695,096\n",
      "Trainable params: 6,695,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# view model summary\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2 \n",
    "    for doc in corpus:\n",
    "        sentence_length = len(doc)\n",
    "        for index, word in enumerate(doc):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([doc[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "# counter = 0\n",
    "# for each in tqdm_notebook(a):\n",
    "#     counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387e7d2550d5420087e5dc1f5cf58a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=522303), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 10000 cumulative loss 72658.8157697916\n",
      "At iteration 20000 cumulative loss 144904.48578679562\n",
      "At iteration 30000 cumulative loss 216680.86483860016\n",
      "At iteration 40000 cumulative loss 291723.59290006757\n",
      "At iteration 50000 cumulative loss 364483.39373322576\n",
      "At iteration 60000 cumulative loss 435933.8864511885\n",
      "At iteration 70000 cumulative loss 507180.83226618916\n",
      "At iteration 80000 cumulative loss 577609.8925533427\n",
      "At iteration 90000 cumulative loss 653302.9688486143\n",
      "At iteration 100000 cumulative loss 731873.7930505725\n",
      "At iteration 110000 cumulative loss 809244.8884728827\n",
      "At iteration 120000 cumulative loss 887483.3859989529\n",
      "At iteration 130000 cumulative loss 962858.6891389012\n",
      "At iteration 140000 cumulative loss 1036665.7088488787\n",
      "At iteration 150000 cumulative loss 1110467.8008727632\n",
      "At iteration 160000 cumulative loss 1182081.934012476\n",
      "At iteration 170000 cumulative loss 1256739.8789029964\n",
      "At iteration 180000 cumulative loss 1334610.2783687362\n",
      "At iteration 190000 cumulative loss 1408784.6237104016\n",
      "At iteration 200000 cumulative loss 1483966.9141400175\n",
      "At iteration 210000 cumulative loss 1558900.9982506512\n",
      "At iteration 220000 cumulative loss 1633296.904443184\n",
      "At iteration 230000 cumulative loss 1707147.3456345766\n",
      "At iteration 240000 cumulative loss 1780640.1232776276\n",
      "At iteration 250000 cumulative loss 1854923.5406056773\n",
      "At iteration 260000 cumulative loss 1926003.9640361047\n",
      "At iteration 270000 cumulative loss 1997083.3639658974\n",
      "At iteration 280000 cumulative loss 2068465.514104366\n",
      "At iteration 290000 cumulative loss 2143521.6087451247\n",
      "At iteration 300000 cumulative loss 2217145.6846626434\n",
      "At iteration 310000 cumulative loss 2290701.5466685016\n",
      "At iteration 320000 cumulative loss 2361625.2953548813\n",
      "At iteration 330000 cumulative loss 2434151.67625029\n",
      "At iteration 340000 cumulative loss 2503982.9018567316\n",
      "At iteration 350000 cumulative loss 2576215.976038983\n",
      "At iteration 360000 cumulative loss 2648817.1080906074\n",
      "At iteration 370000 cumulative loss 2720084.943370367\n",
      "At iteration 380000 cumulative loss 2793416.036171168\n",
      "At iteration 390000 cumulative loss 2863812.7998243193\n",
      "At iteration 400000 cumulative loss 2934715.0853384826\n",
      "At iteration 410000 cumulative loss 3007754.0130387656\n",
      "At iteration 420000 cumulative loss 3078251.998195421\n",
      "At iteration 430000 cumulative loss 3150563.648983326\n",
      "At iteration 440000 cumulative loss 3220503.352277316\n",
      "At iteration 450000 cumulative loss 3291420.824685876\n",
      "At iteration 460000 cumulative loss 3363246.8307901686\n",
      "At iteration 470000 cumulative loss 3434359.207566729\n",
      "At iteration 480000 cumulative loss 3507098.7248039455\n",
      "At iteration 490000 cumulative loss 3576913.376833429\n",
      "At iteration 500000 cumulative loss 3647335.2755601904\n",
      "At iteration 510000 cumulative loss 3717029.1712863995\n",
      "At iteration 520000 cumulative loss 3785335.874289638\n",
      "\n",
      "Epoch: 1 \tTotal Loss: 3800828.0274639684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f151669bda47719e7e9f58cb06dd46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=522303), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 530000 cumulative loss 56750.061166964006\n",
      "At iteration 540000 cumulative loss 130504.27476649646\n",
      "At iteration 550000 cumulative loss 203640.5443869676\n",
      "At iteration 560000 cumulative loss 276743.1440894155\n",
      "At iteration 570000 cumulative loss 346505.78682181914\n",
      "At iteration 580000 cumulative loss 416653.9650032243\n",
      "At iteration 590000 cumulative loss 485539.15683732135\n",
      "At iteration 600000 cumulative loss 552090.8111180415\n",
      "At iteration 610000 cumulative loss 622723.3617439581\n",
      "At iteration 620000 cumulative loss 696779.6769231142\n",
      "At iteration 630000 cumulative loss 770830.7308661082\n",
      "At iteration 640000 cumulative loss 845669.5122698555\n",
      "At iteration 650000 cumulative loss 916580.5551464028\n",
      "At iteration 660000 cumulative loss 988036.5084068746\n",
      "At iteration 670000 cumulative loss 1058498.0451398003\n",
      "At iteration 680000 cumulative loss 1127426.4048933296\n",
      "At iteration 690000 cumulative loss 1197621.8050823081\n",
      "At iteration 700000 cumulative loss 1270781.1695510102\n",
      "At iteration 710000 cumulative loss 1343733.1271472832\n",
      "At iteration 720000 cumulative loss 1414660.3780145827\n",
      "At iteration 730000 cumulative loss 1485180.475742646\n",
      "At iteration 740000 cumulative loss 1555883.3193713436\n",
      "At iteration 750000 cumulative loss 1627311.3505379346\n",
      "At iteration 760000 cumulative loss 1697426.7012615018\n",
      "At iteration 770000 cumulative loss 1768627.9094231916\n",
      "At iteration 780000 cumulative loss 1836467.4962070903\n",
      "At iteration 790000 cumulative loss 1904387.932725328\n",
      "At iteration 800000 cumulative loss 1971562.0897782226\n",
      "At iteration 810000 cumulative loss 2043521.6177574645\n",
      "At iteration 820000 cumulative loss 2113372.0094671417\n",
      "At iteration 830000 cumulative loss 2182856.2872549323\n",
      "At iteration 840000 cumulative loss 2251333.003949942\n",
      "At iteration 850000 cumulative loss 2320841.37561498\n",
      "At iteration 860000 cumulative loss 2387485.3929228177\n",
      "At iteration 870000 cumulative loss 2456691.683518245\n",
      "At iteration 880000 cumulative loss 2525407.460608434\n",
      "At iteration 890000 cumulative loss 2593611.9867830006\n",
      "At iteration 900000 cumulative loss 2662435.9217947316\n",
      "At iteration 910000 cumulative loss 2729150.2738171085\n",
      "At iteration 920000 cumulative loss 2796395.2891081716\n",
      "At iteration 930000 cumulative loss 2865679.906764216\n",
      "At iteration 940000 cumulative loss 2933960.3548997124\n",
      "At iteration 950000 cumulative loss 3001870.263000346\n",
      "At iteration 960000 cumulative loss 3070357.5560061196\n",
      "At iteration 970000 cumulative loss 3137548.9256544667\n",
      "At iteration 980000 cumulative loss 3204647.923162159\n",
      "At iteration 990000 cumulative loss 3273538.3487239378\n",
      "At iteration 1000000 cumulative loss 3343142.5132444655\n",
      "At iteration 1010000 cumulative loss 3409937.5055426033\n",
      "At iteration 1020000 cumulative loss 3476662.670731149\n",
      "At iteration 1030000 cumulative loss 3543822.9111476503\n",
      "At iteration 1040000 cumulative loss 3608462.824864703\n",
      "\n",
      "Epoch: 2 \tTotal Loss: 3639240.5072215046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c8c226e5e74413965e49062d244b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=522303), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 1050000 cumulative loss 37908.91196644679\n",
      "At iteration 1060000 cumulative loss 108821.42273388109\n",
      "At iteration 1070000 cumulative loss 179530.57302266225\n",
      "At iteration 1080000 cumulative loss 248445.48249529622\n",
      "At iteration 1090000 cumulative loss 316751.21700006863\n",
      "At iteration 1100000 cumulative loss 384529.0644330119\n",
      "At iteration 1110000 cumulative loss 450107.9173670053\n",
      "At iteration 1120000 cumulative loss 515770.0750598665\n",
      "At iteration 1130000 cumulative loss 583724.5303665882\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-06e59ed89315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         corpus=wids, window_size=window_size, vocab_size=vocab_size), total=522303):\n\u001b[1;32m      6\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'At iteration {} cumulative loss {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0.0\n",
    "    for x, y in tqdm_notebook(generate_context_word_pairs(\n",
    "        corpus=wids, window_size=window_size, vocab_size=vocab_size), total=522303):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 10000 == 0:\n",
    "            print('At iteration {} cumulative loss {}'.format(i, loss))\n",
    "    # Save the model at each epoch \n",
    "    cbow.save('./models/word2vec_hp_{}.h5'.format(str(epoch)))\n",
    "    print('Epoch:', epoch, '\\tTotal Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
