{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbow_harry_potter.ipynb  harry_potter_data.tar.gz  Reviews.csv\r\n",
      "CBOW_keras.ipynb\t harry_potter.txt\t   Untitled.ipynb\r\n",
      "data\t\t\t India_after_gandhi.txt    word2vec.h5\r\n",
      "final_cbow_hp.ipynb\t models\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = open('harry_potter.txt').read()\n",
    "sentences = sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"They were the last\\npeople you'd expect to be involved in anything strange or mysterious,\\nbecause they just didn't hold with such nonsense.\",\n",
       " 'Mr. Dursley was the director of a firm called Grunnings, which made\\ndrills.',\n",
       " 'He was a big, beefy man with hardly any neck, although he did\\nhave a very large mustache.',\n",
       " 'Mrs. Dursley was thin and blonde and had\\nnearly twice the usual amount of neck, which came in very useful as she\\nspent so much of her time craning over garden fences, spying on the\\nneighbors.',\n",
       " 'The Dursleys had a small son called Dudley and in their\\nopinion there was no finer boy anywhere.',\n",
       " 'The Dursleys had everything they wanted, but they also had a secret, and\\ntheir greatest fear was that somebody would discover it.',\n",
       " \"They didn't\\nthink they could bear it if anyone found out about the Potters.\",\n",
       " \"Mrs.\\nPotter was Mrs. Dursley's sister, but they hadn't met for several years;\\nin fact, Mrs. Dursley pretended she didn't have a sister, because her\\nsister and her good-for-nothing husband were as unDursleyish as it was\\npossible to be.\",\n",
       " 'The Dursleys shuddered to think what the neighbors would\\nsay if the Potters arrived in the street.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our own Stop Words and text cleaning and pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',\n",
    "              'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an',\n",
    "              'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', \n",
    "              'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand',\n",
    "              'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'bottom', 'but', 'by', 'call',\n",
    "              'can', 'cannot', 'cant', 'con', 'could', \"couldnt\", 'cry', 'de', 'describe', 'detail', 'do',\"didn't\", 'done', 'down',\n",
    "              'due', 'during', 'each', \"er\",'eg', 'either', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever',\n",
    "              'every', 'everyone', 'everything', 'everywhere', 'except','few', 'fill', 'find', 'for', 'former', 'formerly',\n",
    "              'found', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', \"hasn't\", 'have', 'he', \n",
    "              'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself',\n",
    "              'his', 'how', 'however', 'i','ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', \"its\", \n",
    "              'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me',\n",
    "              'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', \n",
    "              'my', 'myself', 'namely', 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none', 'noone', 'nor',\n",
    "              'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'only', 'onto', 'or', 'other', \n",
    "              'others', 'otherwise','our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put',\n",
    "              'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show',\n",
    "              'since', 'sincere', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still',\n",
    "              'such', 'system', 'take', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter',\n",
    "              'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'this', 'those', 'though', 'through', 'throughout', \n",
    "              'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'un', 'under', 'until', 'up', 'upon', 'us', \n",
    "              'very', 'via', 'was', 'we', 'well', 'were', 'what',\"whats\", \"what've\", 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter',\n",
    "              'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever',\n",
    "              'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "              \"youd\",'yourselves',\"ill\", \"youll\",\"well\", \"im\", \"youre\", \"were\", \"hes\",\"but\",\"ive\",\" i \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(sentences):\n",
    "    for idx,line in enumerate(sentences):\n",
    "        sentences[idx]= line.replace('\\n', ' ')\n",
    "\n",
    "    for idx,line in enumerate(sentences):\n",
    "        sentences[idx]= line.replace(\"'\", \"\")\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = text_cleaning(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They were the last people youd expect to be involved in anything strange or mysterious, because they just didnt hold with such nonsense.',\n",
       " 'Mr. Dursley was the director of a firm called Grunnings, which made drills.',\n",
       " 'He was a big, beefy man with hardly any neck, although he did have a very large mustache.',\n",
       " 'Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors.',\n",
       " 'The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.',\n",
       " 'The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it.',\n",
       " 'They didnt think they could bear it if anyone found out about the Potters.',\n",
       " 'Mrs. Potter was Mrs. Dursleys sister, but they hadnt met for several years; in fact, Mrs. Dursley pretended she didnt have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be.',\n",
       " 'The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for lines in sentences:\n",
    "    tokens.append(lines.lower().split())\n",
    "\n",
    "for idx, lines in enumerate(tokens):\n",
    "    tokens[idx] = [words for words in lines if words not in stop_words]\n",
    "    \n",
    "for idx, lines in enumerate(tokens):\n",
    "    tokens[idx]= \" \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokens.pickle', 'wb') as f:\n",
    "    pickle.dump(tokens, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokens.pickle', 'rb') as f:\n",
    "    tokens = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing, Building vocabulary and Generating context word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sourav/anaconda3/envs/Tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 20391\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(tokens)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "# build vocabulary of unique words\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in tokens]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 200\n",
    "window_size = 5\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b479325ea234f9d8dd1a97e3799c58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a = generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size)\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "counter = 0\n",
    "for each in tqdm_notebook(a):\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 200)           4078200   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20391)             4098591   \n",
      "=================================================================\n",
      "Total params: 8,176,791\n",
      "Trainable params: 8,176,791\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# view model summary\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515789e146be45ea8d34fad969855520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=483064), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 10000 cumulative loss 84935.79511094093\n",
      "At iteration 20000 cumulative loss 171045.14314365387\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0.0\n",
    "    for x, y in tqdm_notebook(generate_context_word_pairs(\n",
    "        corpus=wids, window_size=window_size, vocab_size=vocab_size), total=483064):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 10000 == 0:\n",
    "            print('At iteration {} cumulative loss {}'.format(i, loss))\n",
    "    # Save the model at each epoch \n",
    "    cbow.save('./models/word2vec_hp_{}.h5'.format(str(epoch)))\n",
    "    print('Epoch:', epoch, '\\tTotal Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
